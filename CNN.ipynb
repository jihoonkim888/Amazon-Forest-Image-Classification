{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import shutil, os\n",
    "import numpy as np\n",
    "# import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Dropout, Flatten, Dense, InputLayer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, Callback, ReduceLROnPlateau\n",
    "# from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.backend import clear_session\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AveragePooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which GPU to use - HEX SERVER ONLY. Go to https://hex.cs.bath.ac.uk/usage for current usage\n",
    "import os \n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "# Prevent GPU memory overflow\n",
    "from tensorflow.compat.v1 import ConfigProto\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "\n",
    "config = ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = InteractiveSession(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import multiprocessing\n",
    "INPUT_SHAPE = (128, 128, 3) # Image Dimensions\n",
    "BATCH_SIZE = 128\n",
    "DROPOUT_RATE = 0.1\n",
    "EPOCHS = 200\n",
    "LR = 0.0001 # Learning Rate\n",
    "REG_STRENGTH = 0.01 # Regularization Strength\n",
    "# NFOLDS = 5 # No of folds for cross validation\n",
    "WORKERS = 16 #multiprocessing.cpu_count()-1 #32 # Multithreading no of threads\n",
    "MAXQ = 10 # Max Queue size for multithreading\n",
    "THRES = [0.2] * 17 # Threshold for truth value of label, applied on sigmoid output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'dataset/'\n",
    "TRAIN_PATH = DATASET_PATH + 'train_file'\n",
    "TEST_PATH = DATASET_PATH + 'test_file'\n",
    "\n",
    "TRAIN_CSV_PATH = DATASET_PATH + '/train_file/train_label.csv'\n",
    "TEST_CSV_PATH = DATASET_PATH + '/test_file/test_label.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(TRAIN_CSV_PATH)\n",
    "df_test = pd.read_csv(TEST_CSV_PATH)\n",
    "\n",
    "df_train['image_name'] = df_train['image_name'].astype(str)\n",
    "df_test['image_name'] = df_test['image_name'].astype(str)\n",
    "\n",
    "df_train['tags'] = df_train['tags'].apply(lambda x: x.split(' '))\n",
    "df_test['tags'] = df_test['tags'].apply(lambda x: x.split(' '))\n",
    "\n",
    "# print(df_train.head())\n",
    "# print()\n",
    "# print(df_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-6b6633285bcc>:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  y_train = np.array(df_train['tags'].tolist()) # image tags (ground truth)\n"
     ]
    }
   ],
   "source": [
    "X_train_files = np.array(df_train['image_name'].tolist()) # filenames\n",
    "X_train_files.reshape((X_train_files.shape[0], 1))\n",
    "y_train = np.array(df_train['tags'].tolist()) # image tags (ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['agriculture', 'artisinal_mine', 'bare_ground', 'blooming', 'blow_down', 'clear', 'cloudy', 'conventional_mine', 'cultivation', 'habitation', 'haze', 'partly_cloudy', 'primary', 'road', 'selective_logging', 'slash_burn', 'water']\n",
      "\n",
      "17 unique tags\n"
     ]
    }
   ],
   "source": [
    "# check labels in tags and how many there are\n",
    "tags = df_train['tags'].values\n",
    "\n",
    "flat_list = [item for sublist in tags for item in sublist]\n",
    "tags_unique, tags_count = np.unique(flat_list, return_counts=True)\n",
    "labels = list(tags_unique)\n",
    "\n",
    "print(labels)\n",
    "print()\n",
    "print(len(labels), \"unique tags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20, 20))\n",
    "# res = [32, 64, 128, 256]\n",
    "# NIMGS = 5\n",
    "# train = df_train.reset_index()\n",
    "# test = df_test.reset_index()\n",
    "\n",
    "# for i in range(len(res)):\n",
    "#     for j in range(NIMGS):\n",
    "#         img = cv2.imread(os.path.join(TRAIN_PATH, df_train['image_name'][j+1]))\n",
    "#         img = cv2.resize(img, (res[i], res[i]))\n",
    "#         plt.subplot(len(res), NIMGS, i*NIMGS+j+1)\n",
    "#         plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "#         plt.title(df_train['tags'][j+1] + \"\\n\" + str(res[i]) + \"x\" + str(res[i]))\n",
    "#         plt.axis('off')\n",
    "    \n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of the labels\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "            \n",
    "plt.bar(range(len(tags_count)), list(tags_count), align='center')\n",
    "plt.xticks(range(len(tags_count)), list(labels), rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define learning curve and evaluation metric (f-2 Beta score):\n",
    "\n",
    "def learning_curve(model_fit, key='acc', ylim=(0.8, 1.01)):\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(model_fit.history[key])\n",
    "    plt.plot(model_fit.history['val_' + key])\n",
    "    plt.title('Learning Curve')\n",
    "    plt.ylabel(key.title())\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylim(ylim)\n",
    "    plt.legend(['train', 'test'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "# F-Beta Score\n",
    "def fbeta_score_K(y_true, y_pred):\n",
    "    beta = 2\n",
    "    beta_squared = beta ** 2\n",
    "    tp = K.sum(y_true * y_pred) + K.epsilon()\n",
    "    fp = K.sum(y_pred) - tp\n",
    "    fn = K.sum(y_true) - tp\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "\n",
    "    result = (beta_squared + 1) * (precision * recall) / (beta_squared * precision + recall + K.epsilon())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 126, 126, 32)      9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 63, 63, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 63, 63, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 61, 61, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 30, 30, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 28, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 26, 26, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 13, 13, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 11, 11, 256)       590080    \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 9, 9, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 17)                69649     \n",
      "=================================================================\n",
      "Total params: 1,979,569\n",
      "Trainable params: 1,979,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Define CNN model architecture:\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(128, 128, 3)),\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPool2D(pool_size=(2, 2)),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPool2D(pool_size=(2, 2)),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "        Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPool2D(pool_size=(2, 2)),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu'),\n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPool2D(pool_size=(2, 2)),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        Flatten(),\n",
    "\n",
    "#         Dense(1024, activation='relu'),\n",
    "        Dense(17, activation='sigmoid') \n",
    "    ])\n",
    "\n",
    "    #optimizer = Adam(0.001, decay=0.0001)\n",
    "\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "clear_session()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25906 validated image filenames belonging to 17 classes.\n",
      "Found 6477 validated image filenames belonging to 17 classes.\n",
      "Found 8096 validated image filenames belonging to 17 classes.\n",
      "Epoch 1/50\n",
      "203/203 [==============================] - 49s 208ms/step - loss: 0.3602 - fbeta_score_K: 0.4938 - val_loss: 0.2428 - val_fbeta_score_K: 0.5547\n",
      "Epoch 2/50\n",
      "203/203 [==============================] - 46s 218ms/step - loss: 0.2239 - fbeta_score_K: 0.5984 - val_loss: 0.2284 - val_fbeta_score_K: 0.5814\n",
      "Epoch 3/50\n",
      "203/203 [==============================] - 46s 218ms/step - loss: 0.2092 - fbeta_score_K: 0.6299 - val_loss: 0.2131 - val_fbeta_score_K: 0.6125\n",
      "Epoch 4/50\n",
      "203/203 [==============================] - 50s 224ms/step - loss: 0.2041 - fbeta_score_K: 0.6395 - val_loss: 0.1955 - val_fbeta_score_K: 0.6540\n",
      "Epoch 5/50\n",
      "203/203 [==============================] - 48s 229ms/step - loss: 0.1993 - fbeta_score_K: 0.6489 - val_loss: 0.1853 - val_fbeta_score_K: 0.6651\n",
      "Epoch 6/50\n",
      "203/203 [==============================] - 46s 219ms/step - loss: 0.1853 - fbeta_score_K: 0.6734 - val_loss: 0.1777 - val_fbeta_score_K: 0.6680\n",
      "Epoch 7/50\n",
      "203/203 [==============================] - 47s 210ms/step - loss: 0.1770 - fbeta_score_K: 0.6894 - val_loss: 0.1725 - val_fbeta_score_K: 0.7063\n",
      "Epoch 8/50\n",
      "203/203 [==============================] - 45s 216ms/step - loss: 0.1715 - fbeta_score_K: 0.7020 - val_loss: 0.1626 - val_fbeta_score_K: 0.7251\n",
      "Epoch 9/50\n",
      "203/203 [==============================] - 45s 215ms/step - loss: 0.1641 - fbeta_score_K: 0.7158 - val_loss: 0.1661 - val_fbeta_score_K: 0.7252\n",
      "Epoch 10/50\n",
      "203/203 [==============================] - 43s 207ms/step - loss: 0.1622 - fbeta_score_K: 0.7212 - val_loss: 0.1525 - val_fbeta_score_K: 0.7286\n",
      "Epoch 11/50\n",
      "203/203 [==============================] - 43s 203ms/step - loss: 0.1571 - fbeta_score_K: 0.7286 - val_loss: 0.1494 - val_fbeta_score_K: 0.7526\n",
      "Epoch 12/50\n",
      "203/203 [==============================] - 44s 207ms/step - loss: 0.1562 - fbeta_score_K: 0.7327 - val_loss: 0.1491 - val_fbeta_score_K: 0.7288\n",
      "Epoch 13/50\n",
      "203/203 [==============================] - 45s 215ms/step - loss: 0.1517 - fbeta_score_K: 0.7396 - val_loss: 0.1434 - val_fbeta_score_K: 0.7397\n",
      "Epoch 14/50\n",
      "203/203 [==============================] - 42s 197ms/step - loss: 0.1504 - fbeta_score_K: 0.7408 - val_loss: 0.1421 - val_fbeta_score_K: 0.7543\n",
      "Epoch 15/50\n",
      "203/203 [==============================] - 43s 203ms/step - loss: 0.1464 - fbeta_score_K: 0.7479 - val_loss: 0.1404 - val_fbeta_score_K: 0.7493\n",
      "Epoch 16/50\n",
      "203/203 [==============================] - 44s 209ms/step - loss: 0.1472 - fbeta_score_K: 0.7478 - val_loss: 0.1394 - val_fbeta_score_K: 0.7619\n",
      "Epoch 17/50\n",
      "203/203 [==============================] - 45s 202ms/step - loss: 0.1453 - fbeta_score_K: 0.7485 - val_loss: 0.1402 - val_fbeta_score_K: 0.7591\n",
      "Epoch 18/50\n",
      "203/203 [==============================] - 41s 194ms/step - loss: 0.1466 - fbeta_score_K: 0.7490 - val_loss: 0.1368 - val_fbeta_score_K: 0.7689\n",
      "Epoch 19/50\n",
      "203/203 [==============================] - 43s 205ms/step - loss: 0.1464 - fbeta_score_K: 0.7493 - val_loss: 0.1356 - val_fbeta_score_K: 0.7717\n",
      "Epoch 20/50\n",
      "203/203 [==============================] - 43s 205ms/step - loss: 0.1430 - fbeta_score_K: 0.7543 - val_loss: 0.1349 - val_fbeta_score_K: 0.7665\n",
      "Epoch 21/50\n",
      "203/203 [==============================] - 42s 203ms/step - loss: 0.1405 - fbeta_score_K: 0.7572 - val_loss: 0.1352 - val_fbeta_score_K: 0.7655\n",
      "Epoch 22/50\n",
      "203/203 [==============================] - 45s 203ms/step - loss: 0.1390 - fbeta_score_K: 0.7602 - val_loss: 0.1346 - val_fbeta_score_K: 0.7637\n",
      "Epoch 23/50\n",
      "203/203 [==============================] - 44s 196ms/step - loss: 0.1386 - fbeta_score_K: 0.7594 - val_loss: 0.1320 - val_fbeta_score_K: 0.7745\n",
      "Epoch 24/50\n",
      "203/203 [==============================] - 42s 201ms/step - loss: 0.1367 - fbeta_score_K: 0.7647 - val_loss: 0.1278 - val_fbeta_score_K: 0.7840\n",
      "Epoch 25/50\n",
      "203/203 [==============================] - 45s 214ms/step - loss: 0.1354 - fbeta_score_K: 0.7687 - val_loss: 0.1291 - val_fbeta_score_K: 0.7728\n",
      "Epoch 26/50\n",
      "203/203 [==============================] - 42s 201ms/step - loss: 0.1356 - fbeta_score_K: 0.7659 - val_loss: 0.1341 - val_fbeta_score_K: 0.7754\n",
      "Epoch 27/50\n",
      "203/203 [==============================] - 42s 199ms/step - loss: 0.1344 - fbeta_score_K: 0.7686 - val_loss: 0.1263 - val_fbeta_score_K: 0.7796\n",
      "Epoch 28/50\n",
      "203/203 [==============================] - 41s 194ms/step - loss: 0.1328 - fbeta_score_K: 0.7712 - val_loss: 0.1270 - val_fbeta_score_K: 0.7829\n",
      "Epoch 29/50\n",
      "203/203 [==============================] - 42s 201ms/step - loss: 0.1340 - fbeta_score_K: 0.7708 - val_loss: 0.1252 - val_fbeta_score_K: 0.7928\n",
      "Epoch 30/50\n",
      "203/203 [==============================] - 43s 205ms/step - loss: 0.1321 - fbeta_score_K: 0.7739 - val_loss: 0.1260 - val_fbeta_score_K: 0.7842\n",
      "Epoch 31/50\n",
      "203/203 [==============================] - 41s 197ms/step - loss: 0.1311 - fbeta_score_K: 0.7748 - val_loss: 0.1223 - val_fbeta_score_K: 0.7856\n",
      "Epoch 32/50\n",
      "203/203 [==============================] - 42s 199ms/step - loss: 0.1306 - fbeta_score_K: 0.7758 - val_loss: 0.1266 - val_fbeta_score_K: 0.7900\n",
      "Epoch 33/50\n",
      "203/203 [==============================] - 43s 206ms/step - loss: 0.1300 - fbeta_score_K: 0.7764 - val_loss: 0.1229 - val_fbeta_score_K: 0.7991\n",
      "Epoch 34/50\n",
      "203/203 [==============================] - 43s 208ms/step - loss: 0.1288 - fbeta_score_K: 0.7793 - val_loss: 0.1227 - val_fbeta_score_K: 0.7905\n",
      "Epoch 35/50\n",
      "203/203 [==============================] - 43s 212ms/step - loss: 0.1292 - fbeta_score_K: 0.7786 - val_loss: 0.1231 - val_fbeta_score_K: 0.7816\n",
      "Epoch 36/50\n",
      "203/203 [==============================] - 43s 204ms/step - loss: 0.1270 - fbeta_score_K: 0.7799 - val_loss: 0.1231 - val_fbeta_score_K: 0.7953\n",
      "Epoch 37/50\n",
      "203/203 [==============================] - 45s 201ms/step - loss: 0.1285 - fbeta_score_K: 0.7802 - val_loss: 0.1234 - val_fbeta_score_K: 0.7981\n",
      "Epoch 38/50\n",
      "203/203 [==============================] - 43s 204ms/step - loss: 0.1257 - fbeta_score_K: 0.7850 - val_loss: 0.1241 - val_fbeta_score_K: 0.8035\n",
      "Epoch 39/50\n",
      "203/203 [==============================] - 43s 206ms/step - loss: 0.1271 - fbeta_score_K: 0.7835 - val_loss: 0.1223 - val_fbeta_score_K: 0.7962\n",
      "Epoch 40/50\n",
      "203/203 [==============================] - 41s 196ms/step - loss: 0.1250 - fbeta_score_K: 0.7846 - val_loss: 0.1198 - val_fbeta_score_K: 0.7938\n",
      "Epoch 41/50\n",
      "203/203 [==============================] - 42s 202ms/step - loss: 0.1243 - fbeta_score_K: 0.7858 - val_loss: 0.1178 - val_fbeta_score_K: 0.8030\n",
      "Epoch 42/50\n",
      "203/203 [==============================] - 42s 201ms/step - loss: 0.1247 - fbeta_score_K: 0.7865 - val_loss: 0.1189 - val_fbeta_score_K: 0.7980\n",
      "Epoch 43/50\n",
      "203/203 [==============================] - 42s 201ms/step - loss: 0.1234 - fbeta_score_K: 0.7876 - val_loss: 0.1177 - val_fbeta_score_K: 0.7968\n",
      "Epoch 44/50\n",
      "203/203 [==============================] - 45s 214ms/step - loss: 0.1212 - fbeta_score_K: 0.7916 - val_loss: 0.1217 - val_fbeta_score_K: 0.8003\n",
      "Epoch 45/50\n",
      "203/203 [==============================] - 47s 212ms/step - loss: 0.1243 - fbeta_score_K: 0.7871 - val_loss: 0.1172 - val_fbeta_score_K: 0.8063\n",
      "Epoch 46/50\n",
      "203/203 [==============================] - 42s 198ms/step - loss: 0.1209 - fbeta_score_K: 0.7924 - val_loss: 0.1206 - val_fbeta_score_K: 0.8066\n",
      "Epoch 47/50\n",
      "203/203 [==============================] - 42s 198ms/step - loss: 0.1230 - fbeta_score_K: 0.7895 - val_loss: 0.1179 - val_fbeta_score_K: 0.8048\n",
      "Epoch 48/50\n",
      "203/203 [==============================] - 41s 196ms/step - loss: 0.1216 - fbeta_score_K: 0.7917 - val_loss: 0.1152 - val_fbeta_score_K: 0.7952\n",
      "Epoch 49/50\n",
      "203/203 [==============================] - 45s 202ms/step - loss: 0.1220 - fbeta_score_K: 0.7908 - val_loss: 0.1148 - val_fbeta_score_K: 0.8118\n",
      "Epoch 50/50\n",
      "203/203 [==============================] - 42s 203ms/step - loss: 0.1195 - fbeta_score_K: 0.7965 - val_loss: 0.1166 - val_fbeta_score_K: 0.8067\n",
      "here dict_keys(['loss', 'fbeta_score_K', 'val_loss', 'val_fbeta_score_K', 'lr'])\n"
     ]
    }
   ],
   "source": [
    "# Run the model\n",
    "\n",
    "X_train_files, X_val_files, y_train, y_val = train_test_split(X_train_files, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "train_df = pd.DataFrame(list(zip(X_train_files, y_train)), columns = ['image_name', 'tags'])\n",
    "val_df = pd.DataFrame(list(zip(X_val_files, y_val)), columns = ['image_name', 'tags'])\n",
    "\n",
    "train_df['tags'] = train_df['tags']\n",
    "val_df['tags'] = val_df['tags']\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df,\n",
    "    directory=TRAIN_PATH,\n",
    "    x_col='image_name',\n",
    "    y_col='tags',\n",
    "    #validate_filenames=False,\n",
    "    target_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]),\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=labels,\n",
    ")\n",
    "#print(\"train gen complete\")\n",
    "val_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_dataframe(\n",
    "    val_df,\n",
    "    directory=TRAIN_PATH,\n",
    "    x_col='image_name',\n",
    "    y_col='tags',\n",
    "    #validate_filenames=False,\n",
    "    target_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]),\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=labels,\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    df_test,\n",
    "    directory=TEST_PATH,\n",
    "    x_col='image_name',\n",
    "    y_col=None,\n",
    "    #validate_filenames=False,\n",
    "    target_size=(INPUT_SHAPE[0], INPUT_SHAPE[1]),\n",
    "    class_mode='categorical',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=labels,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "model_path = 'CNN_weights' + '.h5'\n",
    "\n",
    "# clear_session()\n",
    "# model = create_model()\n",
    "\n",
    "adam = Adam(learning_rate=LR)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=[fbeta_score_K])\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(model_path, monitor='fbeta_score_K', save_best_only=True, mode='max'),\n",
    "    ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3, mode='min', min_lr=0.000001)\n",
    "]\n",
    "\n",
    "history = model.fit(train_generator, epochs=EPOCHS, validation_data=val_generator, callbacks=callbacks,\n",
    "                   workers=WORKERS, use_multiprocessing=False, max_queue_size=MAXQ)\n",
    "\n",
    "print('here', history.history.keys())\n",
    "\n",
    "# DO NOT KNOW WHAT THIS IS FOR?\n",
    "# for key, value in history.history.items():\n",
    "#     fold[num_fold][key] = value \n",
    "\n",
    "model.load_weights(model_path)\n",
    "\n",
    "y_pred = model.predict(test_generator, workers=WORKERS, use_multiprocessing=False, max_queue_size=MAXQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loss</th>\n",
       "      <th>fbeta_score_K</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_fbeta_score_K</th>\n",
       "      <th>lr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.281842</td>\n",
       "      <td>0.541820</td>\n",
       "      <td>0.242846</td>\n",
       "      <td>0.554749</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.221260</td>\n",
       "      <td>0.605311</td>\n",
       "      <td>0.228432</td>\n",
       "      <td>0.581379</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.209153</td>\n",
       "      <td>0.629833</td>\n",
       "      <td>0.213110</td>\n",
       "      <td>0.612543</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.203016</td>\n",
       "      <td>0.640910</td>\n",
       "      <td>0.195541</td>\n",
       "      <td>0.653978</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.195419</td>\n",
       "      <td>0.655334</td>\n",
       "      <td>0.185258</td>\n",
       "      <td>0.665144</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.183448</td>\n",
       "      <td>0.678091</td>\n",
       "      <td>0.177660</td>\n",
       "      <td>0.668013</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.175606</td>\n",
       "      <td>0.694043</td>\n",
       "      <td>0.172489</td>\n",
       "      <td>0.706339</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.169295</td>\n",
       "      <td>0.706461</td>\n",
       "      <td>0.162591</td>\n",
       "      <td>0.725136</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.164864</td>\n",
       "      <td>0.714876</td>\n",
       "      <td>0.166128</td>\n",
       "      <td>0.725193</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.161139</td>\n",
       "      <td>0.722682</td>\n",
       "      <td>0.152536</td>\n",
       "      <td>0.728580</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.156437</td>\n",
       "      <td>0.730511</td>\n",
       "      <td>0.149426</td>\n",
       "      <td>0.752649</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.153727</td>\n",
       "      <td>0.735322</td>\n",
       "      <td>0.149079</td>\n",
       "      <td>0.728757</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.150967</td>\n",
       "      <td>0.740310</td>\n",
       "      <td>0.143383</td>\n",
       "      <td>0.739688</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.150580</td>\n",
       "      <td>0.740971</td>\n",
       "      <td>0.142050</td>\n",
       "      <td>0.754312</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.147541</td>\n",
       "      <td>0.746260</td>\n",
       "      <td>0.140378</td>\n",
       "      <td>0.749314</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.145783</td>\n",
       "      <td>0.749356</td>\n",
       "      <td>0.139360</td>\n",
       "      <td>0.761887</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.144796</td>\n",
       "      <td>0.750049</td>\n",
       "      <td>0.140224</td>\n",
       "      <td>0.759106</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.144190</td>\n",
       "      <td>0.751686</td>\n",
       "      <td>0.136770</td>\n",
       "      <td>0.768888</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.145065</td>\n",
       "      <td>0.750910</td>\n",
       "      <td>0.135572</td>\n",
       "      <td>0.771748</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.141906</td>\n",
       "      <td>0.755392</td>\n",
       "      <td>0.134944</td>\n",
       "      <td>0.766503</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.141861</td>\n",
       "      <td>0.755831</td>\n",
       "      <td>0.135232</td>\n",
       "      <td>0.765461</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.140421</td>\n",
       "      <td>0.758910</td>\n",
       "      <td>0.134582</td>\n",
       "      <td>0.763713</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.137740</td>\n",
       "      <td>0.762647</td>\n",
       "      <td>0.132021</td>\n",
       "      <td>0.774490</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.137884</td>\n",
       "      <td>0.762627</td>\n",
       "      <td>0.127821</td>\n",
       "      <td>0.784037</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.136578</td>\n",
       "      <td>0.765331</td>\n",
       "      <td>0.129095</td>\n",
       "      <td>0.772767</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.135553</td>\n",
       "      <td>0.766413</td>\n",
       "      <td>0.134096</td>\n",
       "      <td>0.775392</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.134063</td>\n",
       "      <td>0.769456</td>\n",
       "      <td>0.126268</td>\n",
       "      <td>0.779621</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.133335</td>\n",
       "      <td>0.770514</td>\n",
       "      <td>0.127013</td>\n",
       "      <td>0.782879</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.133400</td>\n",
       "      <td>0.770756</td>\n",
       "      <td>0.125210</td>\n",
       "      <td>0.792847</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.132009</td>\n",
       "      <td>0.773636</td>\n",
       "      <td>0.126035</td>\n",
       "      <td>0.784195</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.131428</td>\n",
       "      <td>0.774732</td>\n",
       "      <td>0.122314</td>\n",
       "      <td>0.785589</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.131603</td>\n",
       "      <td>0.774486</td>\n",
       "      <td>0.126617</td>\n",
       "      <td>0.789993</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.129988</td>\n",
       "      <td>0.776553</td>\n",
       "      <td>0.122908</td>\n",
       "      <td>0.799136</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.129135</td>\n",
       "      <td>0.779068</td>\n",
       "      <td>0.122666</td>\n",
       "      <td>0.790470</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.129048</td>\n",
       "      <td>0.779073</td>\n",
       "      <td>0.123125</td>\n",
       "      <td>0.781618</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.127365</td>\n",
       "      <td>0.781326</td>\n",
       "      <td>0.123104</td>\n",
       "      <td>0.795283</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.126798</td>\n",
       "      <td>0.782202</td>\n",
       "      <td>0.123352</td>\n",
       "      <td>0.798076</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.125610</td>\n",
       "      <td>0.784725</td>\n",
       "      <td>0.124104</td>\n",
       "      <td>0.803473</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.126164</td>\n",
       "      <td>0.784307</td>\n",
       "      <td>0.122292</td>\n",
       "      <td>0.796213</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.124945</td>\n",
       "      <td>0.785722</td>\n",
       "      <td>0.119764</td>\n",
       "      <td>0.793836</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.125079</td>\n",
       "      <td>0.785494</td>\n",
       "      <td>0.117756</td>\n",
       "      <td>0.803010</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.123914</td>\n",
       "      <td>0.787797</td>\n",
       "      <td>0.118851</td>\n",
       "      <td>0.798028</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.123364</td>\n",
       "      <td>0.788643</td>\n",
       "      <td>0.117660</td>\n",
       "      <td>0.796798</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.123478</td>\n",
       "      <td>0.789165</td>\n",
       "      <td>0.121734</td>\n",
       "      <td>0.800304</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.124112</td>\n",
       "      <td>0.787462</td>\n",
       "      <td>0.117191</td>\n",
       "      <td>0.806307</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.122480</td>\n",
       "      <td>0.790255</td>\n",
       "      <td>0.120631</td>\n",
       "      <td>0.806570</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.122818</td>\n",
       "      <td>0.789570</td>\n",
       "      <td>0.117871</td>\n",
       "      <td>0.804781</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.121923</td>\n",
       "      <td>0.792110</td>\n",
       "      <td>0.115176</td>\n",
       "      <td>0.795190</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.122012</td>\n",
       "      <td>0.790657</td>\n",
       "      <td>0.114808</td>\n",
       "      <td>0.811795</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.120748</td>\n",
       "      <td>0.793956</td>\n",
       "      <td>0.116593</td>\n",
       "      <td>0.806716</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        loss  fbeta_score_K  val_loss  val_fbeta_score_K      lr\n",
       "0   0.281842       0.541820  0.242846           0.554749  0.0001\n",
       "1   0.221260       0.605311  0.228432           0.581379  0.0001\n",
       "2   0.209153       0.629833  0.213110           0.612543  0.0001\n",
       "3   0.203016       0.640910  0.195541           0.653978  0.0001\n",
       "4   0.195419       0.655334  0.185258           0.665144  0.0001\n",
       "5   0.183448       0.678091  0.177660           0.668013  0.0001\n",
       "6   0.175606       0.694043  0.172489           0.706339  0.0001\n",
       "7   0.169295       0.706461  0.162591           0.725136  0.0001\n",
       "8   0.164864       0.714876  0.166128           0.725193  0.0001\n",
       "9   0.161139       0.722682  0.152536           0.728580  0.0001\n",
       "10  0.156437       0.730511  0.149426           0.752649  0.0001\n",
       "11  0.153727       0.735322  0.149079           0.728757  0.0001\n",
       "12  0.150967       0.740310  0.143383           0.739688  0.0001\n",
       "13  0.150580       0.740971  0.142050           0.754312  0.0001\n",
       "14  0.147541       0.746260  0.140378           0.749314  0.0001\n",
       "15  0.145783       0.749356  0.139360           0.761887  0.0001\n",
       "16  0.144796       0.750049  0.140224           0.759106  0.0001\n",
       "17  0.144190       0.751686  0.136770           0.768888  0.0001\n",
       "18  0.145065       0.750910  0.135572           0.771748  0.0001\n",
       "19  0.141906       0.755392  0.134944           0.766503  0.0001\n",
       "20  0.141861       0.755831  0.135232           0.765461  0.0001\n",
       "21  0.140421       0.758910  0.134582           0.763713  0.0001\n",
       "22  0.137740       0.762647  0.132021           0.774490  0.0001\n",
       "23  0.137884       0.762627  0.127821           0.784037  0.0001\n",
       "24  0.136578       0.765331  0.129095           0.772767  0.0001\n",
       "25  0.135553       0.766413  0.134096           0.775392  0.0001\n",
       "26  0.134063       0.769456  0.126268           0.779621  0.0001\n",
       "27  0.133335       0.770514  0.127013           0.782879  0.0001\n",
       "28  0.133400       0.770756  0.125210           0.792847  0.0001\n",
       "29  0.132009       0.773636  0.126035           0.784195  0.0001\n",
       "30  0.131428       0.774732  0.122314           0.785589  0.0001\n",
       "31  0.131603       0.774486  0.126617           0.789993  0.0001\n",
       "32  0.129988       0.776553  0.122908           0.799136  0.0001\n",
       "33  0.129135       0.779068  0.122666           0.790470  0.0001\n",
       "34  0.129048       0.779073  0.123125           0.781618  0.0001\n",
       "35  0.127365       0.781326  0.123104           0.795283  0.0001\n",
       "36  0.126798       0.782202  0.123352           0.798076  0.0001\n",
       "37  0.125610       0.784725  0.124104           0.803473  0.0001\n",
       "38  0.126164       0.784307  0.122292           0.796213  0.0001\n",
       "39  0.124945       0.785722  0.119764           0.793836  0.0001\n",
       "40  0.125079       0.785494  0.117756           0.803010  0.0001\n",
       "41  0.123914       0.787797  0.118851           0.798028  0.0001\n",
       "42  0.123364       0.788643  0.117660           0.796798  0.0001\n",
       "43  0.123478       0.789165  0.121734           0.800304  0.0001\n",
       "44  0.124112       0.787462  0.117191           0.806307  0.0001\n",
       "45  0.122480       0.790255  0.120631           0.806570  0.0001\n",
       "46  0.122818       0.789570  0.117871           0.804781  0.0001\n",
       "47  0.121923       0.792110  0.115176           0.795190  0.0001\n",
       "48  0.122012       0.790657  0.114808           0.811795  0.0001\n",
       "49  0.120748       0.793956  0.116593           0.806716  0.0001"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_history = pd.DataFrame(history.history)\n",
    "train_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_history.to_csv('CNN_train_history.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_history['fbeta_score_K'])\n",
    "plt.plot(train_history['val_fbeta_score_K'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('fbeta_score_K')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(train_history['loss'])\n",
    "plt.plot(train_history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes labeled predictions with threshold\n",
    "\n",
    "pred = []\n",
    "\n",
    "for row in y_test:\n",
    "    label = []\n",
    "    for i, acc in enumerate(row):\n",
    "        if acc >= THRES[0]:\n",
    "            label.append(labels[i])\n",
    "    pred.append(label)\n",
    "    \n",
    "print(pred[0])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test['tags'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "def calc_acc(df_test):\n",
    "    test = df_test['tags']\n",
    "    pred = df_test['pred_tags'].apply(lambda x: x.split(' '))\n",
    "    \n",
    "    from sklearn.preprocessing import MultiLabelBinarizer\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    test = mlb.fit_transform(test)\n",
    "    pred = mlb.transform(pred)\n",
    "    score = fbeta_score(test,pred,beta=2,average='weighted')\n",
    "    return score\n",
    "\n",
    "acc_score = calc_acc(df_test)\n",
    "print(\"Accuracy Score: \", acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
